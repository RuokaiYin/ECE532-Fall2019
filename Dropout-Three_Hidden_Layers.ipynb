{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.special import expit as activation_function # the logistic function\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10): # default values of params\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "# this class creates a NeuralNetwork\n",
    "class NeuralNetwork: \n",
    "    def __init__(self, \n",
    "                 no_of_in_nodes, \n",
    "                 no_of_out_nodes,  \n",
    "                 no_of_hidden_nodes_1,\n",
    "                 no_of_hidden_nodes_2,\n",
    "                 no_of_hidden_nodes_3, \n",
    "                 learning_rate, \n",
    "                 bias=None # default value of bias\n",
    "                ):  \n",
    "        # initialize the neural network\n",
    "        self.no_of_in_nodes = no_of_in_nodes\n",
    "        self.no_of_out_nodes = no_of_out_nodes       \n",
    "        self.no_of_hidden_nodes_1 = no_of_hidden_nodes_1  \n",
    "        self.no_of_hidden_nodes_2 = no_of_hidden_nodes_2\n",
    "        self.no_of_hidden_nodes_3 = no_of_hidden_nodes_3\n",
    "        self.learning_rate = learning_rate \n",
    "        self.bias = bias\n",
    "        self.create_weight_matrices() # call method to create a weight matrix\n",
    "        \n",
    "        \n",
    "    def create_weight_matrices(self):\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        # print(X)\n",
    "        \n",
    "        bias_node = 1 if self.bias else 0 # if bias exists, the value of bias node is 1\n",
    "\n",
    "        # construct the matrix btw input layer and the first hidden layer\n",
    "        n = (self.no_of_in_nodes + bias_node) * self.no_of_hidden_nodes_1 # the total entries of the matrix btw input and hidden \n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.wih = X.rvs(n).reshape((self.no_of_hidden_nodes_1, \n",
    "                                                   self.no_of_in_nodes + bias_node)) # rvs()?\n",
    "\n",
    "        # construct the matrix btw the first hidden layer and the second hidden layer\n",
    "        n = (self.no_of_hidden_nodes_1 + bias_node) * self.no_of_hidden_nodes_2\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.wh1h2 = X.rvs(n).reshape((self.no_of_hidden_nodes_2, \n",
    "                                                    (self.no_of_hidden_nodes_1 + bias_node))) # rvs()?\n",
    "        \n",
    "        # construct the matrix btw the second hidden layer and the third hidden layer\n",
    "        n = (self.no_of_hidden_nodes_2 + bias_node) * self.no_of_hidden_nodes_3\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.wh2h3 = X.rvs(n).reshape((self.no_of_hidden_nodes_3, \n",
    "                                                    (self.no_of_hidden_nodes_2 + bias_node))) # rvs()?\n",
    "        \n",
    "        # construct the matrix btw the third hidden layer and the output layer\n",
    "        n = (self.no_of_hidden_nodes_3 + bias_node) * self.no_of_out_nodes\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.who = X.rvs(n).reshape((self.no_of_out_nodes, \n",
    "                                                    (self.no_of_hidden_nodes_3 + bias_node))) # rvs()?\n",
    "#DONE\n",
    "    def dropout_weight_matrices(self,\n",
    "                                active_input_percentage=0.70,\n",
    "                                active_hidden_percentage=0.70):\n",
    "        # restore wih array, if it had been used for dropout\n",
    "        self.wih_orig = self.wih.copy()\n",
    "        self.no_of_in_nodes_orig = self.no_of_in_nodes\n",
    "        \n",
    "        # restore wh1h2 array\n",
    "        self.wh1h2_orig = self.wh1h2.copy()\n",
    "        self.no_of_hidden_nodes_1_orig = self.no_of_hidden_nodes_1\n",
    "        \n",
    "        # restore wh2h3 array\n",
    "        self.wh2h3_orig = self.wh2h3.copy()\n",
    "        self.no_of_hidden_nodes_2_orig = self.no_of_hidden_nodes_2\n",
    "        \n",
    "        # restore who array\n",
    "        self.who_orig = self.who.copy() \n",
    "        self.no_of_hidden_nodes_3_orig = self.no_of_hidden_nodes_3\n",
    "# DONE      \n",
    "     \n",
    "        # randomly dropout nodes in input layer \n",
    "        active_input_nodes = int(self.no_of_in_nodes * active_input_percentage) # number of active input nodes\n",
    "        active_input_indices = sorted(random.sample(range(0, self.no_of_in_nodes), # random choose the active nodes\n",
    "                                      active_input_nodes))\n",
    "        \n",
    "        # randomly dropout nodes in the first hidden layer\n",
    "        active_hidden_nodes_1 = int(self.no_of_hidden_nodes_1 * active_hidden_percentage)\n",
    "        active_hidden_indices_1 = sorted(random.sample(range(0, self.no_of_hidden_nodes_1), \n",
    "                                       active_hidden_nodes_1))\n",
    "        \n",
    "        # randomly dropout nodes in the second hidden layer\n",
    "        active_hidden_nodes_2 = int(self.no_of_hidden_nodes_2 * active_hidden_percentage)\n",
    "        active_hidden_indices_2 = sorted(random.sample(range(0, self.no_of_hidden_nodes_2), \n",
    "                                       active_hidden_nodes_2))\n",
    "        \n",
    "        # randomly dropout nodes in the third hidden layer\n",
    "        active_hidden_nodes_3 = int(self.no_of_hidden_nodes_3 * active_hidden_percentage)\n",
    "        active_hidden_indices_3 = sorted(random.sample(range(0, self.no_of_hidden_nodes_3), \n",
    "                                       active_hidden_nodes_3))\n",
    "# DONE       \n",
    "        \n",
    "        # construct the dropout matrix wih\n",
    "        # ignore an input node is equivalent to delete a column of wih\n",
    "        # ignore a hidden node is equivalent to delete a row of wih \n",
    "        self.wih = self.wih[:, active_input_indices][active_hidden_indices_1]  \n",
    "        \n",
    "         # construct the dropout matrix wh1h2\n",
    "        self.wh1h2 = self.wh1h2[:, active_hidden_indices_1][active_hidden_indices_2]   \n",
    "        \n",
    "         # construct the dropout matrix wh2h3\n",
    "        self.wh2h3 = self.wh2h3[:, active_hidden_indices_2][active_hidden_indices_3]\n",
    "        \n",
    "        # construct the dropout matrix who\n",
    "        # ignore a hidden node is equivalent to delete a column of who\n",
    "        self.who = self.who[:, active_hidden_indices_3]\n",
    "        \n",
    "        # update the number of nodes in input layer and hidden layer after dropout\n",
    "        self.no_of_in_nodes = active_input_nodes\n",
    "        self.no_of_hidden_nodes_1 = active_hidden_nodes_1\n",
    "        self.no_of_hidden_nodes_2 = active_hidden_nodes_2\n",
    "        self.no_of_hidden_nodes_3 = active_hidden_nodes_3\n",
    "        \n",
    "        return active_input_indices, active_hidden_indices_1, active_hidden_indices_2, active_hidden_indices_3 \n",
    "# DONE   \n",
    "    \n",
    "    # ?????\n",
    "    def weight_matrices_reset(self, \n",
    "                              active_input_indices, \n",
    "                              active_hidden_indices_1,\n",
    "                              active_hidden_indices_2,\n",
    "                              active_hidden_indices_3 ):\n",
    "        \n",
    "        \"\"\"\n",
    "        self.wih and self.who contain the newly adapted values from the active nodes.\n",
    "        We have to reconstruct the original weight matrices by assigning the new values \n",
    "        from the active nodes\n",
    "        \"\"\"\n",
    "        temp = self.wih_orig.copy()[:,active_input_indices]\n",
    "        temp[active_hidden_indices_1] = self.wih\n",
    "        self.wih_orig[:, active_input_indices] = temp\n",
    "        self.wih = self.wih_orig.copy()\n",
    "        \n",
    "        temp = self.wh1h2_orig.copy()[:,active_hidden_indices_1]\n",
    "        temp[active_hidden_indices_2] = self.wh1h2\n",
    "        self.wh1h2_orig[:, active_hidden_indices_1] = temp\n",
    "        self.wh1h2 = self.wh1h2_orig.copy()\n",
    "        \n",
    "        temp = self.wh2h3_orig.copy()[:,active_hidden_indices_2]\n",
    "        temp[active_hidden_indices_3] = self.wh2h3\n",
    "        self.wh2h3_orig[:, active_hidden_indices_2] = temp\n",
    "        self.wh2h3 = self.wh2h3_orig.copy()\n",
    "\n",
    "        self.who_orig[:, active_hidden_indices_3] = self.who\n",
    "        self.who = self.who_orig.copy()\n",
    "        \n",
    "        self.no_of_in_nodes = self.no_of_in_nodes_orig\n",
    "        self.no_of_hidden_nodes_1 = self.no_of_hidden_nodes_1_orig\n",
    "        self.no_of_hidden_nodes_2 = self.no_of_hidden_nodes_2_orig\n",
    "        self.no_of_hidden_nodes_3 = self.no_of_hidden_nodes_3_orig\n",
    "# DONE\n",
    "           \n",
    "    def train_single(self, input_vector, target_vector):\n",
    "        \"\"\" \n",
    "        input_vector and target_vector can be tuple, list or ndarray\n",
    "        \"\"\"\n",
    " \n",
    "        if self.bias:\n",
    "            # add a bias node to end of input layer\n",
    "            input_vector = np.concatenate( (input_vector, [self.bias]) )\n",
    "\n",
    "        input_vector = np.array(input_vector, ndmin=2).T # the minimum demision is 2 # transpose\n",
    "        target_vector = np.array(target_vector, ndmin=2).T # target vector contains the correct rsults\n",
    "\n",
    "        output_vector1 = np.dot(self.wih, input_vector) # get the dot product of input vector and wih\n",
    "        output_vector_hidden_1 = activation_function(output_vector1) # get nodes in hidden layer using logistic function\n",
    "        \n",
    "        if self.bias:\n",
    "            # add a bias node to end of the first hidden layer\n",
    "            output_vector_hidden_1 = np.concatenate( (output_vector_hidden_1, [[self.bias]]) )\n",
    "        output_vector2 = np.dot(self.wh1h2, output_vector_hidden_1)\n",
    "        output_vector_hidden_2 = activation_function(output_vector2)\n",
    "        \n",
    "        if self.bias:\n",
    "            output_vector_hidden_2 = np.concatenate( (output_vector_hidden_2, [[self.bias]]) )\n",
    "        output_vector3 = np.dot(self.wh2h3, output_vector_hidden_2)\n",
    "        output_vector_hidden_3 = activation_function(output_vector3)   \n",
    "       \n",
    "        if self.bias:\n",
    "            output_vector_hidden_3 = np.concatenate( (output_vector_hidden_3, [[self.bias]]) )\n",
    "        output_vector4 = np.dot(self.who, output_vector_hidden_3) # get the dot product of hidden vector and who\n",
    "        output_vector_network = activation_function(output_vector4) # get nodes in output layer using logistic function\n",
    "# DONE   \n",
    "\n",
    "        # calculate the error of output layer\n",
    "        output_errors = target_vector - output_vector_network # compute the error\n",
    "        # update the weights between the first hidden layer and output layer:\n",
    "        tmp = output_errors * output_vector_network * (1.0 - output_vector_network) # ????? \n",
    "        tmp = self.learning_rate  * np.dot(tmp, output_vector_hidden_3.T) # ?????\n",
    "        self.who += tmp \n",
    "\n",
    "        # calculate error of the third hidden layer:\n",
    "        hidden_errors_3 = np.dot(self.who.T, output_errors) # ?????\n",
    "        # update the weights:\n",
    "        tmp = hidden_errors_3 * output_vector_hidden_3 * (1.0 - output_vector_hidden_3) # ?????\n",
    "        if self.bias:\n",
    "            x = np.dot(tmp, output_vector_hidden_2.T)[:-1,:] \n",
    "        else:\n",
    "            x = np.dot(tmp, output_vector_hidden_2.T)\n",
    "        self.wh2h3 += self.learning_rate * x # ?????\n",
    "        \n",
    "        # calculate error of the second hidden layer:\n",
    "        hidden_errors_2 = np.dot(self.wh2h3.T, hidden_errors_3) # ?????\n",
    "        # update the weights:\n",
    "        tmp = hidden_errors_2 * output_vector_hidden_2 * (1.0 - output_vector_hidden_2) # ?????\n",
    "        if self.bias:\n",
    "            x = np.dot(tmp, output_vector_hidden_1.T)[:-1,:] \n",
    "        else:\n",
    "            x = np.dot(tmp, output_vector_hidden_1.T)\n",
    "        self.wh1h2 += self.learning_rate * x # ????\n",
    "        \n",
    "        # calculate error of the first hidden layer:\n",
    "        hidden_errors_1 = np.dot(self.wh1h2.T, hidden_errors_2) # ?????\n",
    "        # update the weights:\n",
    "        tmp = hidden_errors_1 * output_vector_hidden_1 * (1.0 - output_vector_hidden_1) # ?????\n",
    "        if self.bias:\n",
    "            x = np.dot(tmp, input_vector.T)[:-1,:] \n",
    "        else:\n",
    "            x = np.dot(tmp, input_vector.T)\n",
    "        self.wih += self.learning_rate * x # ?????\n",
    "# DONE\n",
    "                   \n",
    "    def train(self, data_array, \n",
    "              labels_one_hot_array, \n",
    "              epochs=1,\n",
    "              active_input_percentage=0.70,\n",
    "              active_hidden_percentage=0.70,\n",
    "              no_of_dropout_tests = 10):\n",
    "\n",
    "        partition_length = int(len(data_array) / no_of_dropout_tests) # divide training data into 10 folds\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch: \", epoch)\n",
    "            for start in range(0, len(data_array), partition_length): # partition_length is the step size \n",
    "                active_in_indices, active_hidden_indices_1, active_hidden_indices_2, active_hidden_indices_3 = \\\n",
    "                           self.dropout_weight_matrices(active_input_percentage,\n",
    "                                                        active_hidden_percentage)\n",
    "                \n",
    "                for i in range(start, start + partition_length): # do training for each fold\n",
    "                    self.train_single(data_array[i][active_in_indices], \n",
    "                                     labels_one_hot_array[i]) \n",
    "                # reconstruct the original weight matrix \n",
    "                self.weight_matrices_reset(active_in_indices, active_hidden_indices_1, \\\n",
    "                                           active_hidden_indices_2, active_hidden_indices_3 )\n",
    "                     \n",
    "# DONE\n",
    "\n",
    "    # obtain the training result (output verctor)\n",
    "    def run(self, input_vector):\n",
    "        # input_vector can be tuple, list or ndarray\n",
    "        \n",
    "        if self.bias:\n",
    "            # adding bias node to the end of the input_vector\n",
    "            input_vector = np.concatenate( (input_vector, [self.bias]) )\n",
    "            \n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "\n",
    "        output_vector = np.dot(self.wih, input_vector)\n",
    "        output_vector = activation_function(output_vector)\n",
    "        \n",
    "        if self.bias:\n",
    "            output_vector = np.concatenate( (output_vector, [[self.bias]]) )     \n",
    "        output_vector = np.dot(self.wh1h2, output_vector)\n",
    "        output_vector = activation_function(output_vector)\n",
    "        \n",
    "        if self.bias:\n",
    "            output_vector = np.concatenate( (output_vector, [[self.bias]]) )\n",
    "        output_vector = np.dot(self.wh2h3, output_vector)\n",
    "        output_vector = activation_function(output_vector)\n",
    "        \n",
    "        if self.bias:\n",
    "            output_vector = np.concatenate( (output_vector, [[self.bias]]) )\n",
    "        output_vector = np.dot(self.who, output_vector)\n",
    "        output_vector = activation_function(output_vector)\n",
    "    \n",
    "        return output_vector\n",
    "    \n",
    "    # count the numer of correct & wrong classifications\n",
    "    def evaluate(self, data, labels):\n",
    "        corrects, wrongs = 0, 0\n",
    "        for i in range(len(data)):\n",
    "            res = self.run(data[i])\n",
    "            res_max = res.argmax()\n",
    "            if res_max == labels[i]:\n",
    "                corrects += 1\n",
    "            else:\n",
    "                wrongs += 1\n",
    "        return corrects, wrongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data from csv file\n",
    "image_size = 28 # width and length\n",
    "no_of_different_labels = 10 #  i.e. 0, 1, 2, 3, ..., 9\n",
    "image_pixels = image_size * image_size\n",
    "\n",
    "# get data of the training set\n",
    "train_data = np.loadtxt(\"mnist_train.csv\", \n",
    "                        delimiter=\",\")\n",
    "train_data = train_data[0:10000, :]\n",
    "\n",
    "\n",
    "# get data of the test set\n",
    "test_data = np.loadtxt(\"mnist_test.csv\", \n",
    "                       delimiter=\",\") \n",
    "test_data = test_data[0:1000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 785)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "\n",
    "# We will map these values into an interval from [0.01, 1] by multiplying each pixel by 0.99 / 255 \n",
    "# and adding 0.01 to the result. This way, we avoid 0 values as inputs.\n",
    "fac = 0.99 / 255\n",
    "train_imgs = np.asfarray(train_data[:, 1:]) * fac + 0.01\n",
    "test_imgs = np.asfarray(test_data[:, 1:]) * fac + 0.0\n",
    "\n",
    "\n",
    "train_labels = np.asfarray(train_data[:, :1]) # the correct classification of each image in training set\n",
    "test_labels = np.asfarray(test_data[:, :1]) # the correct classification of each image in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the labelled images into one-hot representation\n",
    "lr = np.arange(no_of_different_labels)\n",
    "\n",
    "# transform labels into one hot representation\n",
    "train_labels_one_hot = (lr==train_labels).astype(np.float)\n",
    "test_labels_one_hot = (lr==test_labels).astype(np.float)\n",
    "\n",
    "# we don't want zeroes and ones in the labels neither:\n",
    "train_labels_one_hot[train_labels_one_hot==0] = 0.01\n",
    "train_labels_one_hot[train_labels_one_hot==1] = 0.99\n",
    "test_labels_one_hot[test_labels_one_hot==0] = 0.01\n",
    "test_labels_one_hot[test_labels_one_hot==1] = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "0 1000\n",
      "1000 2000\n",
      "2000 3000\n",
      "3000 4000\n",
      "4000 5000\n",
      "5000 6000\n",
      "6000 7000\n",
      "7000 8000\n",
      "8000 9000\n",
      "9000 10000\n"
     ]
    }
   ],
   "source": [
    "parts = 10\n",
    "partition_length = int(len(train_imgs) / parts)\n",
    "print(partition_length)\n",
    "\n",
    "start = 0\n",
    "for start in range(0, len(train_imgs), partition_length):\n",
    "    print(start, start + partition_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "epoch:  1\n",
      "epoch:  2\n",
      "accruracy train:  0.1001\n",
      "accruracy: test 0.085\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "simple_network = NeuralNetwork(no_of_in_nodes = image_pixels, \n",
    "                               no_of_out_nodes = 10, \n",
    "                               no_of_hidden_nodes_1 = 500,\n",
    "                               no_of_hidden_nodes_2 = 500,\n",
    "                               no_of_hidden_nodes_3 = 500,\n",
    "                               learning_rate = 0.01)\n",
    "    \n",
    "simple_network.train(train_imgs, \n",
    "                     train_labels_one_hot, \n",
    "                     active_input_percentage=1,\n",
    "                     active_hidden_percentage=1,\n",
    "                     no_of_dropout_tests = 50,\n",
    "                     epochs=epochs)\n",
    "\n",
    "corrects, wrongs = simple_network.evaluate(train_imgs, train_labels)\n",
    "print(\"accruracy train: \", corrects / ( corrects + wrongs))\n",
    "corrects, wrongs = simple_network.evaluate(test_imgs, test_labels)\n",
    "print(\"accruracy: test\", corrects / ( corrects + wrongs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
